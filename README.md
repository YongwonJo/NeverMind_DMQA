# NeverMind_DMQA
This repository contains key summary and description files for various papers.

## Members
The member consists of 8 researchers belonging to the Data Mining & Quality Analytics (DMQA) lab. [[website]](http://dmqa.korea.ac.kr/) \
**If you have any questions, please contact us by email of the researcher who prepared the material.**

* Young Jae Lee
  * Ph.D. Student / Email: jae601@korea.ac.kr
  
* Yongwon Jo
  * Ph.D. Student / Email: gyj4318@korea.ac.kr
  
* Jinhyeok Park
  * Ph.D. Student / Email: vc2013@korea.ac.kr
  
* Jaehoon Kim
  * Ph.D. Student / Email: jhoon0418@korea.ac.kr
  
* Saerin Lim
  * M.S. Student / Email: momo_om@korea.ac.kr
  
* Jongkook Heo
  * M.S. Student / Email: hjkso1406@korea.ac.kr
  
* Eunji Koh
  * M.S. Student / Email: ejkoh21@korea.ac.kr
  
* Leekyung Yoo
  * M.S. Student / Email: ylk0801@korea.ac.kr

## Vision Transformer
Key Summary and Description of Paper on Vision Transformer

* 2021-07-02
  * Young Jae Lee / Visual Transformers: Token-based Image Representation and Processing for Computer Vision [[paper]](https://arxiv.org/abs/2006.03677)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210702/%5B20210702%5DVisual%20Transformers%20-%20Token-based%20Image%20Representation%20and%20Processing%20for%20Computer%20Vision.pdf)
  * Saerin Lim / TransGAN: Two Transformers Can Make One Strong GAN [[paper]](https://arxiv.org/abs/2102.07074)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210702/%5B20210702%5DTransGAN-Two%20Transformers%20Can%20Make%20One%20Strong%20GAN.pdf)


* 2021-07-09
  * Jaehoon Kim / Rethinking Spatial Dimensions of Vision Transformers [[paper]](https://arxiv.org/abs/2103.16302)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210709/%5B20210709%5DRethinking%20Spatial%20Dimensions%20of%20Vision%20Transformers.pdf)
  * Jongkook Heo / Emerging Properties in Self-Supervised Vision Transformers [[paper]](https://arxiv.org/abs/2104.14294)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210709/%5B20210709%5DEmerging%20Properties%20in%20Self-Supervised%20Vision%20Transformer.pdf)

* 2021-07-16
  * Yongwon Jo / Inpainting Transformer for Anomaly Detection [[paper]](https://arxiv.org/abs/2104.13897)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210716/%5B20210716%5D%20Inpainting%20Tranformer%20for%20Anomaly%20Detection.pdf)
  * Eunji Koh / Training Data-Efficient Image Transformers & Distillation through Attention [[paper]](http://proceedings.mlr.press/v139/touvron21a.html)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210716/%5B20210716%5DTraining%20data-efficient%20image%20transformers%20%26%20distillation%20through%20attention.pdf)

* 2021-07-23
  * Jinhyeok Park / End-to-End Object Detection with Transformers [[paper]](https://arxiv.org/abs/2005.12872)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210723/%5B20210723%5DEnd%20to%20End%20Object%20Detection%20with%20Transformers.pdf)
  * Leekyung Yoo / Mlp-Mixer: An All-Mlp Architecture for Vision [[paper]](https://arxiv.org/abs/2105.01601)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210723/%5B20210723%5DMLP-Mixer%20-%20An%20all-MLP%20Architecture%20for%20Vision.pdf)

* 2021-08-06
  * Young Jae Lee / Swin Transformer: Hierarchical Vision Transformer using Shifted Windows [[paper]](https://arxiv.org/abs/2103.14030)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210806/%5B20210806%5DSwin%20Transformer%20-%20Hierarchical%20Vision%20Transformer%20using%20Shifted%20Windows.pdf)
  * Saerin Lim / SiT: Self-supervised vIsion Transformer [[paper]](https://arxiv.org/abs/2104.03602)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210806/%5B20210806%5DSiT%20-%20Self-supervised%20vision%20Transformer.pdf)

* 2021-08-13
  * Jaehoon Kim / Efficient Self-supervised Vision Transformers for Representation Learning [[paper]](https://arxiv.org/abs/2106.09785)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210813/%5B20210813%5DEfficient%20Self-supervised%20Vision%20Transformers%20for%20Representation%20Learning.pdf)
  * Jongkook Heo / XCiT: Cross-Covariance Image Transformers [[paper]](https://arxiv.org/abs/2106.09681)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210813/%5B20210813%5D%20XCiT-%20Cross-Covariance%20Image%20Transformers.pdf)

* 2021-08-20
  * Yongwon Jo / When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations [[paper]](https://arxiv.org/abs/2106.01548)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210820/%5B20210820%5DWhen%20Vision%20Trnasformers%20Outperform%20ResNets%20without%20Pretraining.pdf)
  * Eunji Koh / ViViT: A Video Vision Transformer [[paper]](https://arxiv.org/abs/2103.15691)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210820/%5B20210820%5DViViT_A%20Video%20Vision%20Transformer.pdf)

* 2021-08-27
  * Jinhyeok Park / MOTR: End-to-End Multiple-Object Tracking with TRansformer [[paper]](https://arxiv.org/abs/2105.03247)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210827/%5B20210827%5DMOTR%20-%20End-to-End%20Multiple-Object%20Tracking%20with%20TRansformer.pdf)
  * Leekyung Yoo / CvT: Introducing Convolutions to Vision Transformers [[paper]](https://arxiv.org/abs/2103.15808)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210827/%5B20210827%5DCvT%20-%20Introducing%20Convolutions%20to%20Vision%20Transformers.pdf)

* 2021-09-10
  * Young Jae Lee / Self-Supervised Learning with Swin Transformers [[paper]](https://arxiv.org/abs/2105.04553)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210910/%5B20210910%5DSelf-Supervised%20Learning%20with%20Swin%20Transformers.pdf)
  * Saerin Lim / PSViT: Better Vision Transformer via Token Pooling and Attention Sharing [[paper]](https://arxiv.org/abs/2108.03428)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210910/%5B20210910%5DPSViT%20Better%20Vision%20Transformer%20via%20Token%20Pooling%20and%20Attention%20Sharing.pdf)

* 2021-09-17
  * Jaehoon Kim / Focal Self-attention for Local-Global Interactions in Vision Transformers [[paper]](https://arxiv.org/abs/2107.00641)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210917/%5B20210917%5DFocal%20Self-attention%20for%20Local-Global%20Interactions%20in%20Vision%20Transformers.pdf)
  * Jongkook Heo / Going deeper with Image Transformers [[paper]](https://arxiv.org/abs/2103.17239)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210917/%5B20210917%5DCaiT-%20Going%20deeper%20with%20Image%20Transformers.pdf)

* 2021-09-24
  * Yongwon Jo / Skeletor: Skeletal Transformers for Robust Body-Pose Estimation [[paper]](https://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/html/Jiang_Skeletor_Skeletal_Transformers_for_Robust_Body-Pose_Estimation_CVPRW_2021_paper.html)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210924/%5B20210924%5DSkeletor%3DSkeletal%20Transformers%20for%20Robust%20Body-Pose%20Estimation.pdf)
  * Eunji Koh / How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers [[paper]](https://arxiv.org/abs/2106.10270)[[presentation]](https://github.com/dudwojae/NeverMind_DMQA/blob/main/VisionTransformer/20210924/%5B20210924%5DHow%20to%20train%20your%20ViT.%20Data%2C%20Augmentation%2C%20and%20Regularization%20in%20Vision%20Transformers.pdf)

* 2021-10-01
  * Jinhyeok Park /
  * Leekyung Yoo /
